{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "HTML(\"<style>\" + open(\"style.css\").read() + \"</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"headline\">\n",
    "Language Technology / Sprachtechnologie\n",
    "<br><br>\n",
    "Wintersemester 2021/2022\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"description\">\n",
    "    Übung zum Thema <i id=\"topic\">\"N-grams\"</i>\n",
    "    <br><br>\n",
    "    Deadline Abgabe: <i #id=\"submission\">Friday, 12.11.2021 (11:55 Uhr)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Präsenzübung\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organization\n",
    "\n",
    "Each week you will receive a handout with some theoretical and/ or programming tasks related to NLP. You can also work on them at home, but be aware that in the practice class you have the opportunity to discuss and ask questions. <br>\n",
    "Each handout also contains homework. Working on these homework tasks and handing them in is not mandatory, but you can benefit from that in two ways: You get feedback on your solutions and a better understanding of the subject, and, more importantly, you have the chance to improve your final grade for the exam. If you hand in your solutions, we will grade them based on their quality. For each homework task there are points to collect depending on the amount\n",
    "of work you have to put into it. You will find the maximum number of points to gain for a task in its header. <br>\n",
    "Finally, if you want to hand in your solutions, please use the Moodle system. Please only submit Jupyter notebooks.\n",
    "The deadline for the homework is XXX. Later submissions will not be graded.\n",
    "\n",
    "\n",
    "### Installation\n",
    "\n",
    "In order to be able to complete the exercises and the homework, you need to install Python (http://www.python.org) and Jupyter (https://jupyter.org). \n",
    "As we are working with the Natural Language Toolkit (NLTK), please also install NLTK (http://www.nltk.org) and its packages.\n",
    "\n",
    "For further information also refer to: <br>\n",
    "https://www.nltk.org/install.html <br>\n",
    "http://www.nltk.org/data.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.corpus import*\n",
    "from nltk.book import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.1:</i> <br>\n",
    "</div>\n",
    "\n",
    "Discuss in which of the following cases a frequency distribution may be used reasonably:\n",
    "1. To save a list of tokens.\n",
    "2. To count, how often each word type occurs in a given document.\n",
    "3. To find collocations in a text.\n",
    "4. To count, how often adjectives, nouns or verbs occur in a given text.\n",
    "5. To compute the number of occurrences of each item in a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "1. is false. As in a frequency distribution the keys are stored as a set, multiple occurrences of a certain token will be merged into a single key. Additionally, the original ordering of the tokens will be lost.\n",
    "2. is true. This is one of the main uses of a FD, e.g. initialize it with a list of words.\n",
    "3. is true. If the FD is initialized with bigrams, it can be used to analyze which bigrams occur most often in a document.\n",
    "4. is true. For this purpose, the FD should be initialized with the part-of-speech tags from a document, e.g. initialize it with ['PRP','VBP','TO','VB']\n",
    "5. is true. We can initialize the FD with the list and later output the list contents by just printing the FD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.2:</i> <br>\n",
    "</div>\n",
    "\n",
    "Discuss in which of the following cases a conditional frequency distribution may be used reasonably:\n",
    "1. To count how often each word type occurs in a given document, which belongs to a given category.\n",
    "2. To find collocations in a text.\n",
    "3. To count how often adjectives, nouns or verbs occur in a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "1. is true. We need to (a) initialize Conditional Frequency Distribution (CFD) with (category,word) pairs;\n",
    "2. is true. We could e.g. use the left token in a bigram as the condition and then store a frequency distribution of all tokens that appear on the right of this token. However, the focus in using the conditional frequency distribution instead of a frequency distribution only is slightly different, e.g. we may use it to find all collocations with ’red’ being the first word.\n",
    "3. is true. For a simple count it would be better to use a normal frequency distribution, but a CFD would allow us e.g. to store how often certain tokens are used in a certain word class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.3:</i> <br>\n",
    "</div>\n",
    "\n",
    "Token/Type/Vocabulary: Which of the following statements are true?\n",
    "1. Every token is a type.\n",
    "2. The vocabulary of a text consists of all tokens.\n",
    "3. The vocabulary of a text consists of all types.\n",
    "4. The vocabulary of a text consists of the union of all tokens and all types.\n",
    "5. The vocabulary of a text consists of the intersection of all tokens and all types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "1. Since there exist different definitions for the terms token and type, a simple true or false is not appropriate. We define a token as a sequence of characters, including punctuation, excluding whitespace. Different capitalization of the same word, e.g. “car” and “Car” are regarded as different types. Different appearances of the same token, e.g. “car” are collapsed to one type “car”. A token is a particular appearance of a given word in a text, i.e. the LIST (or sequence) of tokens represents the text, while the SET of tokens represents the vocabulary and an element in this set is called a type. Consider e.g. the string “1 2 3 4 5 1 2 3 4 5”. There are 10 tokens, but there are only 5 types.\n",
    "2. False.\n",
    "3. True.\n",
    "4. If you speak of a union, you speak of sets and not of lists (within the context of lists you speak of appending or merging). So, strictly if you take the set of all tokens you will end up with the vocabulary already (the elements of this set are called types). If you take a union of a set with itself, you end up with the set itself. Under this consideration, you could answer true.\n",
    "5. Same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.4:</i> <br>\n",
    "</div>\n",
    "\n",
    "Lists in Python: Let l and m be lists of words. Which of the following lines are syntactically correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: l.index(\"hi\") \n",
    "#2: l[-2]\n",
    "#3: x = l + m\n",
    "#4: x = l & m\n",
    "#5: l[1,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "1. Is syntactically correct, but it will throw a ValueError, if \"hi\" is not in the list l.\n",
    "2. Is syntactically correct, as lists are cylic in Python, but it will throw an IndexError if the list l has less than 2 elements.\n",
    "3. Is syntactically correct. It will append the list m to the list l and assigns the resulting list to the variable x.\n",
    "4. Throws a TypeError. Remember: It will work for sets and & means to intersect two sets.\n",
    "5. Throws a TypeErrpr. Remember: l[1:4] represents a subset of a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.5:</i> <br>\n",
    "</div>\n",
    "\n",
    "Explore some texts provided by NLTK (to avoid slow reactions do the following with a sublist of text1, i.e.\n",
    "t = text1[:500]) and explain the meaning of the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = text1[:500]\n",
    "print(str(\"len:\"),len(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "len(t)- length of t;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sorted(t):\",sorted(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "sorted(t) - sorted list of words in t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"set(t):\",set(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "set(t) - set of words in t, a set has no double entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sorted(set(t)):\",sorted(set(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "sorted(set(t)) - sorted returns a list, even if a set is given as an argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"len(set(t)):\",len(set(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "len(set(t)) - length of set of words used in t, that is the length of vocabulary used in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find out which types of the sentence \"Today it's nice weather. Is it not nice today :-)?\" are contained in the Chat Corpus of NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weatherTokens = [\"Today\",\"it´s\", \"nice\", \"weather\" ,\".\" ,\"Is\" ,\"it\", \"not\" ,\"nice\" ,\"today\", \":-)\", \"?\"]\n",
    "set1 = set(weatherTokens)\n",
    "set2 = set(text5)\n",
    "print (set1 & set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.6:</i> <br>\n",
    "</div>\n",
    "\n",
    "*Task 2.6.1:* Explain the following function, what does it compute? (Level 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in text1.tokens if w.lower().startswith('e')]\n",
    "print(set(words[:10]))\n",
    "print('count:', len(words))\n",
    "print('count:', len(set(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "    \n",
    "The function calculates how many words starting with the letter e are contained in a corpus. It is not important if the 'e' is in upper or lower case as all words are converted to lower case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.6.2:* Change the function above so that it extracts all words from a corpus that start and end with the letter 's' and consists out of 4 letters, the output should contain all found words and the count of them. (Level 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in text1.tokens if w.lower().startswith('s') and w.lower().endswith('s') and len(w) == 4]\n",
    "print(set(words[:10]))\n",
    "print('count:', len(words))\n",
    "print('count:', len(set(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.6.3:* Another way of finding interesting tokens in a corpus is to see which ones do not occur in another corpus. Write a method that prints all tokens that only appear in a certain corpus, given another corpus. (Level 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = set(text3)\n",
    "set2 = set(text5)\n",
    "print (set1 - set2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.7:</i> <br>\n",
    "</div>\n",
    "\n",
    "*Task 2.7.1:* Use a frequency distribution and print the vocabulary of Moby Dick (text1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(text1)\n",
    "print(fdist)\n",
    "vocabulary = fdist.keys()\n",
    "print (vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.7.2:* Find the 20 most and least frequent words in Moby Dick (text1). Using the command fdist.hapaxes() you can also find words that only appear once (hapaxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(text1)\n",
    "print(\"most common: \", fdist.most_common(20))\n",
    "print(\"least common: \", fdist.most_common()[:-20-1:-1])\n",
    "print(\"hapaxes: \", fdist.hapaxes()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.8:</i> <br>\n",
    "</div>\n",
    "\n",
    "\n",
    "*Task 2.8.1:* Write a funtion that computes how many times the word \"lol\" appears in the chat corpus (text 5) and how much this is as a percentage of the total number of words in this text. (Level 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text5.count('lol'))\n",
    "print(text5.count('lol') / len(text5)*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.8.2:* Adapt the code from task X.X to find the 20 most common words in a corpus starting with 'th'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without FreqDist\n",
    "words = [w for w in text2.tokens if w.lower().startswith('th')]\n",
    "fdist = FreqDist(words)\n",
    "print(\"most common: \", fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.8.3:* We can also look at the distribution of word lengths in a text by creating a FreqDist out of a list of numbers, with each number being the length of the corresponding word in the text. <br>\n",
    "Compare the count and frequency of the most frequent word length in Moby Dick (text1) to those in Monty Python and the Holy Grail (text6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(len(w) for w in text1)  \n",
    "print(fdist.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that the most frequent word length is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist[3])\n",
    "print(fdist.freq(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that 50223 (or 20%) of the words in Moby Dick have three letters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist2 = FreqDist(len(w) for w in text6) \n",
    "print(fdist.max())\n",
    "print(fdist2[fdist2.max()])\n",
    "print(fdist2.freq((fdist2.max())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Monty Python and the Holy Grail 5982 occurences of three-letter-words can be counted, but they make up for 35% of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.9:</i> <br>\n",
    "</div>\n",
    "\n",
    "Write a function get_word_frequency that extracts the most frequent word with `n` letters from a corpus `c`. (Level 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent_with_n_letters(n,c):\n",
    "    cfd = nltk.ConditionalFreqDist((len(word), word) for word in c.tokens)\n",
    "    return cfd[n].most_common(1)\n",
    "    \n",
    "print(most_frequent_with_n_letters(3, text1))\n",
    "print(most_frequent_with_n_letters(4, text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.10:</i> <br>\n",
    "</div>\n",
    "\n",
    "One of the corpora in NLTK is the US Presidential Inaugural Addresses. An interesting property of\n",
    "this collection is its time dimension: The corpus contains an address for each president."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.10.1:* Take a look at the function below. Without executing the code on your PC, describe what the resulting chart shows. Then execute it to verify your answer. (Level 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((len(w), fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for w in inaugural.raw(fileid))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "The function plots how many tokens with duplicates were contained in each inaugural address over the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.10.2:* Enhance the function above so that it plots the number of distinct tokens (without duplicates) for each speech. (Level 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((len(w), fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for w in set(inaugural.raw(fileid)))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.10.3:* Let’s look at how the words “America” and “citizen” are used over time. Use a CFD and plot how often each of those two words are used in each inaugural address. (Level 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (target, fileid[:4])\n",
    "    for fileid in inaugural.fileids()\n",
    "    for w in inaugural.words(fileid)\n",
    "    for target in ['america', 'citizen']\n",
    "    if w.lower().startswith(target))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.11:</i> <br>\n",
    "</div>\n",
    "\n",
    "*Task 2.11.1:* Pick 20 first names randomly – male and female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.11.2:* Fill in the table below: <br>\n",
    "\n",
    "| - | male   | female | sum\n",
    "|------|------|------|-----\n",
    "|  ends in 'a' | ? | ? | ?\n",
    "|  ends not with an 'a' | ? | ? | ?\n",
    "|  sum | ? | ? | ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong> \n",
    "    \n",
    "We pick up the names of politicians and actresses: Winston, Clement, Anthony, Harold, Alexander, Harold, Edward, James, Margaret, John, T ony, Alice, N ancy, Cristen, Dolly, Pamela, Judith, Jennifer, Elizabeth, Tina\n",
    "\n",
    "| - | male   | female | sum\n",
    "|------|------|------|-----\n",
    "|  ends in 'a' | 0 | 3 | 3\n",
    "|  ends not with an 'a' | 10 | 7 | 17\n",
    "|  sum | 10 | 10 | 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.11.3:* Calculate the probability, that a name which ends in ’a’ is a female name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong> \n",
    "\n",
    "$P(female|ends with 'a') = \\frac{Number Female Names Ending With A}{Number Male And Female Names Ending With A} $<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.11.4:* Use the conditional frequency distribution over the Names corpus to verify the hypothesis that first names ending with an ‘a’ are most likely female. (Level 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong> \n",
    "\n",
    "The solution is given in the following listing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist((gender,name[-1])\n",
    "    for gender in names.fileids()\n",
    "    for name in names.words(gender)\n",
    "    )\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice from the plot that the names ending with 'a' are mostly female."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.12:</i> <br>\n",
    "</div>\n",
    "\n",
    "Using the given conditional frequency distribution over bigrams from the Brown corpus, complete the method\n",
    "generate so that it generates sentences given a certain target word. Then change the underlying corpus\n",
    "(e.g. use the Book of Genesis corpus) and compare the results. (Level 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a distribution of bigrams\n",
    "cfd = nltk.ConditionalFreqDist([(w1, w2) for (w1, w2) in nltk.bigrams(brown.words())])\n",
    "cfd['house']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(cfd, start, length):\n",
    "    current = start\n",
    "    for i in range(length):\n",
    "        print(current)\n",
    "        (next_word, freq) = cfd[current].most_common(1)[0]\n",
    "        current = next_word\n",
    "    print('---')\n",
    "              \n",
    "generate(cfd, 'I', 10)\n",
    "generate(cfd, 'house', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.13:</i> <br>\n",
    "</div>\n",
    "\n",
    "Compute the probability of the sentence \"the city is old\" under bigram models from two different nltk corpora. Under which corpus is the sentence more probable? Can you find (trial and error) a sentence that is more likely for the other corpus without being directly contained in it?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd_1 = nltk.ConditionalFreqDist([(w1, w2) for (w1, w2) in nltk.bigrams(genesis.words())])\n",
    "cfd_2 = nltk.ConditionalFreqDist([(w1, w2) for (w1, w2) in nltk.bigrams(gutenberg.words())])\n",
    "\n",
    " \n",
    "def computeBigramProbability(cfd, wordlist):\n",
    "    probability = 1\n",
    "    for i in range(len(wordlist)-1):\n",
    "        w1 = wordlist[i]\n",
    "        w2 = wordlist[i+1]\n",
    "        #print(cfd[w1])\n",
    "        bigramProb = cfd[w1].freq(w2)\n",
    "        print(w1, w2, bigramProb)\n",
    "        probability = probability*bigramProb\n",
    "    print(\"overall probability: \", probability)\n",
    "\n",
    "sentence = [\"the\", \"city\", \"is\", \"old\"]\n",
    "\n",
    "\n",
    "computeBigramProbability(cfd_1, sentence)\n",
    "computeBigramProbability(cfd_2, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.14:</i> <br>\n",
    "</div>\n",
    "\n",
    "Consider the following text: “Today it’s nice weather. Is it not nice today :-) ?” <br>\n",
    "1. What is the length of this text? What may be counted?\n",
    "2. What are reasonable tokens?\n",
    "3. State the types and the vocabulary of this short text according to your chosen tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong> \n",
    "\n",
    "1. It is not obvious what to count and what to regard as a token. One may be interested in the number of characters including or excluding white spaces. In addition \":-)\"may be regarded as one token (smiley) or as 3 tokens. If you use len(\"Today it's ...\") you will get 51, as all characters including whitespaces are count. If you use NLTK to first split the string into tokens and then count its items, you will get 14. You could also use the function split to produce the tokens, which will result in 11 tokens. See the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Today it's nice weather. Is it not nice today :-) ?\"\n",
    "wordsNLTK = nltk.word_tokenize(text)\n",
    "print(wordsNLTK)\n",
    "len(wordsNLTK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsSplit = text.split()\n",
    "print(wordsSplit)\n",
    "len(wordsSplit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. As discussed earlier there is more than one possible representation. We include punctuation and split the text into the following tokens: Today, it’s, nice, weather, ., Is, it, not, nice, today, :-), ?\n",
    "3. We get the following types: 'Today',\"it's\",'nice','weather','Is','it','not','today',':-)','?' vocabulary: set([’Today’, ït’s\", ’nice’, ’weather’, ’.’, ’Is’, ’it’, ’not’, ’today’, ’:-)’, ’?’])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 2.15:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.15.1:* Take a look at the code below. Without executing it on your computer, what is the output? (Level 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Hello. This is a text. A text that contains sentences, which contain words. It has no greater meaning!\"\n",
    "for stc in nltk.sent_tokenize(s):\n",
    "    print(stc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code gets each sentence from s seperately and prints it out:<br>\n",
    "Hello.<br>\n",
    "This is a text.<br>\n",
    "A text that contains sentences, which contain words.<br>\n",
    "It has no greater meaning!<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.15.2:* Change the function above so that it extracts all tokens contained in String s. (Hint: Use the segmenter!) (Level 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Hello. This is a text. A text that contains sentences, which contain words. It has no greater meaning!\"\n",
    "for w in nltk.word_tokenize(s):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.15.3:* Change the document in order to make it particularly difficult for the tokenizer to work correctly. Run it again and check how well it performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong style=\"color: blue\">Lösung:</strong>\n",
    "\n",
    "Depends on how creative you are :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hausübung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Guidlines\n",
    "* The submission has to be done by a team of two people. **Individual submissions will not be graded.**\n",
    "* Please state **the name and matriculation number of all team members in every submission** clearly.\n",
    "* Only **one team member** should submit the homework. If more than one version of the same homework is submitted by accident (submitted by more than one group member), please reach out to a tutor **as soon as possible**. Otherwise, the first submitted homework will be graded.\n",
    "* The submission must be in a Jupyter Notebook format (.ipynb). Submissions in other formats will **not be graded**.\n",
    "* It is not necessary to also submit the part of the exercise discussed by the tutor, please only submit the homework part.\n",
    "* If pictures need to be submitted, it is allowed to hand them in in a zip folder, together with the notebook. They should be added to the notebook like this: <br> *!\\[example1](examplepicture1.PNG)* (without apostrophs in a Markdown-Cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Homework 2.1:</i>\n",
    "        ::: 6 Homework points :::</div>\n",
    "\n",
    "Implement a language guesser, i.e. a function that takes a given text and outputs the language it thinks the text is written in. The function should base its decision on the frequency of individual characters in each language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.1.1:* Implement a function get_language_cfd(languages, words) which takes a list of languages as an argument and returns a conditional frequency distribution where:\n",
    "* the languages are the conditions\n",
    "* the values are the lower case characters found in the words for each language <br>\n",
    "Inside the function you can access the UDHR corpus to get samples for several languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.1.2:* Develop an algorithm which calculates the overall score of a given text based on the frequency of characters accessible by language_model_cfd[language].freq(character). Implement a function guessLanguage that returns the most likely language for a given text according to your algorithm from the previous sub task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.1.3:* Test your implementation with the data text1, text2 and text3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your function does not detect the correct language for at least two of these sentences, improve your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_cfd(languages, words):\n",
    "    \"\"\"Build a ConditionalFrequencyDistribution of character frequencies in the UDHR corpus conditioned \n",
    "    on each language\"\"\"\n",
    "    \n",
    "\n",
    "def guess_language(language_model_cfd, text):\n",
    "    \"\"\"Returns the guessed language for the given text\"\"\"\n",
    "\n",
    "    \n",
    "languages = ['English', 'German_Deutsch', 'French_Francais']\n",
    "# build the language models\n",
    "# udhr contains the Universal Declaration of Human Rights in over 300 languages\n",
    "language_base = dict((language, udhr.words(language + '-Latin1')) for language in languages)\n",
    "language_model_cfd = get_language_cfd(languages, language_base)\n",
    "\n",
    "# print the models for visual inspection (you always should have a look at the data :)\n",
    "for language in languages:\n",
    "    for key in language_model_cfd[language].keys():\n",
    "        print(language, key, \"->\", language_model_cfd[language].freq(key))\n",
    "  \n",
    "text1 = \"Peter had been to the office before they arrived.\"\n",
    "text2 = \"Si tu finis tes devoirs, je te donnerai des bonbons.\"\n",
    "text3 = \"Das ist ein schon recht langes deutsches Beispiel.\"\n",
    "\n",
    "# guess the language by comparing the frequency distributions\n",
    "print()\n",
    "print(guess_language(language_model_cfd, text1)) # English 2.88\n",
    "print()\n",
    "print(guess_language(language_model_cfd, text2)) # French_Francais 2.74\n",
    "print()\n",
    "print(guess_language(language_model_cfd, text3)) # German_Deutsch 3.03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.1.4:* Discuss why English and German texts are difficult to distinguish with the given approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Homework 2.2:</i>\n",
    "        ::: 4 Homework points :::</div>\n",
    "\n",
    "The previous language guesser was based on the frequency of characters. Implement alternative language guesser based on the following lexical units:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.2.1:* tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.2.2:* character bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task 2.2.3:* token bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Homework 2.3:</i>\n",
    "        ::: 1 extra exam bonus point :::</div>\n",
    "\n",
    "We will evaluate your system on unknown data. 1 extra exam bonus point will be awarded to the 3 teams who submitted the best system. The final results will be presented in the lecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
